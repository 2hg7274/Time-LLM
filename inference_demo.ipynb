{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897b8a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/2hg_time-llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-17 14:17:15,294] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/2hg_time-llm/compiler_compat/ld: cannot find -laio: 그런 파일이나 디렉터리가 없습니다\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/user/anaconda3/envs/2hg_time-llm/compiler_compat/ld: cannot find -lcufile: 그런 파일이나 디렉터리가 없습��다\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from peft import PeftModel\n",
    "from models.TimeLLM import Model as TimeLLMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f59b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1. Config 정의 ──────────────────────────────────────────────────────\n",
    "class Config:\n",
    "    # 태스크 및 시퀀스 길이\n",
    "    task_name = \"long_term_forecast\"\n",
    "    seq_len   = 512\n",
    "    pred_len  = 96\n",
    "\n",
    "    # 입력 변수 수 (univariate 시계열이면 1)\n",
    "    enc_in = 21\n",
    "\n",
    "    # 내부 차원\n",
    "    d_model    = 32    # PatchEmbedding 차원\n",
    "    d_ff       = 32    # ReprogrammingLayer 내부 FF 차원\n",
    "    llm_dim    = 5120   # LLM 임베딩 차원\n",
    "    llm_layers = 48    # LLM 레이어 수\n",
    "    n_heads    = 8\n",
    "\n",
    "    # 패치 설정\n",
    "    patch_len = 16\n",
    "    stride    = 8\n",
    "\n",
    "    # 드롭아웃\n",
    "    dropout = 0.1\n",
    "\n",
    "    # (프롬프트 도메인 사용 안 함)\n",
    "    prompt_domain = False\n",
    "    content       = \"TimeLLM-Weather\"\n",
    "\n",
    "    # LoRA 설정 (여기서는 False로 두고, 이후 llm_model만 패치)\n",
    "    lora               = False\n",
    "    lora_r             = 8\n",
    "    lora_alpha         = 32\n",
    "    lora_dropout       = 0.1\n",
    "    lora_target_modules= [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
    "\n",
    "    llm_model=\"QWEN\"\n",
    "\n",
    "configs = Config()\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ad2984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:03<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# ── 2. TimeLLM 모델 인스턴스화 ────────────────────────────────────────\n",
    "base_model = TimeLLMModel(configs).to(device)\n",
    "base_model = base_model.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e0e36e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (llm_model): PeftModelForCausalLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(152064, 5120)\n",
       "        (layers): ModuleList(\n",
       "          (0-47): 48 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (patch_embedding): PatchEmbedding(\n",
       "    (padding_patch_layer): ReplicationPad1d()\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (mapping_layer): Linear(in_features=152064, out_features=1000, bias=True)\n",
       "  (reprogramming_layer): ReprogrammingLayer(\n",
       "    (query_projection): Linear(in_features=32, out_features=256, bias=True)\n",
       "    (key_projection): Linear(in_features=5120, out_features=256, bias=True)\n",
       "    (value_projection): Linear(in_features=5120, out_features=256, bias=True)\n",
       "    (out_projection): Linear(in_features=256, out_features=5120, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (output_projection): FlattenHead(\n",
       "    (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "    (linear): Linear(in_features=2048, out_features=96, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (normalize_layers): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ── 3. LoRA 어댑터만 llm_model에 패치 (체크포인트 경로를 실제로 맞춰주세요) ──\n",
    "lora_checkpoint = \"/home/user/2HG/Time-LLM/checkpoints/long_term_forecast_weather_512_96_TimeLLM_Weather_ftM_sl512_ll48_pl96_dm32_nh8_el2_dl1_df32_fc3_ebtimeF_test_0-TimeLLM-Weather/checkpoint_epoch2_iter9000\"\n",
    "base_model.llm_model = PeftModel.from_pretrained(\n",
    "    base_model.llm_model,\n",
    "    lora_checkpoint,\n",
    ")\n",
    "model = base_model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "785814a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4. 입력 데이터 로드 & 히스토리 준비 ───────────────────────────────\n",
    "df = pd.read_csv(\"/home/user/2HG/Time-LLM/dataset/weather/weather_inf2.csv\", parse_dates=True, index_col=0)\n",
    "# (1) multivariate 히스토리: 마지막 seq_len 행, 모든 컬럼\n",
    "history = df.values[-configs.seq_len:]       # shape (seq_len, n_vars)\n",
    "x_enc = torch.tensor(history, dtype=torch.float32) \\\n",
    "             .unsqueeze(0)  # → (1, seq_len, n_vars)\n",
    "x_enc = x_enc.to(device)\n",
    "\n",
    "# (2) 마킹/디코더 입력은 사용되지 않으므로 더미로 생성\n",
    "#     shape (1, seq_len, 1) / (1, pred_len, 1) 모두 가능\n",
    "x_mark_enc = torch.zeros(1, configs.seq_len, 1).to(device)\n",
    "x_dec      = torch.zeros(1, configs.pred_len, 1).to(device)\n",
    "x_mark_dec = torch.zeros(1, configs.pred_len, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fdde01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 5. 순전파로 예측 수행 ────────────────────────────────────────────\n",
    "with torch.no_grad():\n",
    "    # y_pred shape = (batch=1, pred_len, enc_in)\n",
    "    y_pred = model(x_enc, x_mark_enc, x_dec, x_mark_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4497314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next 96‑step forecast (shape=(96, 21)):\n",
      "[[985.88947   18.878614 293.1527   ... 480.17798   28.947504 413.87958 ]\n",
      " [986.3342    19.138489 293.5139   ... 563.99603   29.509834 415.8713  ]\n",
      " [986.4221    19.337273 293.63867  ... 627.1677    30.106148 416.59427 ]\n",
      " ...\n",
      " [985.91986   18.77389  293.01065  ... 466.6192    29.458714 414.2543  ]\n",
      " [985.7716    18.735102 292.87674  ... 413.6166    29.244934 412.8206  ]\n",
      " [985.3192    18.23087  292.48715  ... 205.30408   28.696547 411.22394 ]]\n"
     ]
    }
   ],
   "source": [
    "# ── 6. 결과 NumPy 배열로 변환 ────────────────────────────────────────\n",
    "forecast = y_pred.squeeze(0).cpu().numpy()  # (pred_len,)\n",
    "\n",
    "print(\"Next 96‑step forecast (shape={}):\".format(forecast.shape))\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9adfee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 7. DataFrame 생성 ───────────────────────────────────────────────────────────\n",
    "# (1) 원본 df에서 컬럼명 가져오기\n",
    "orig_df = pd.read_csv(\"/home/user/2HG/Time-LLM/dataset/weather/weather_inf2.csv\", index_col=0, parse_dates=True)\n",
    "cols    = orig_df.columns.tolist()\n",
    "\n",
    "# (2) 예측 결과로 DataFrame 만들기\n",
    "#     만약 시간 인덱스를 그대로 이어가고 싶다면 아래 주석을 참고하세요.\n",
    "forecast_df = pd.DataFrame(forecast, columns=cols)\n",
    "\n",
    "# ── (선택) datetime index 이어 붙이기 ───────────────────────────────────────\n",
    "# 원본 인덱스가 DatetimeIndex이고 freq 정보가 있다면:\n",
    "last_ts = orig_df.index[-1]\n",
    "freq    = orig_df.index.freq or (orig_df.index[-1] - orig_df.index[-2])\n",
    "future_idx = pd.date_range(start=last_ts + freq, periods=forecast.shape[0], freq=freq)\n",
    "forecast_df.index = future_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "644278cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Forecast saved to weather_forecast.csv\n"
     ]
    }
   ],
   "source": [
    "# ── 8. CSV로 저장 ─────────────────────────────────────────────────────────────\n",
    "out_path = \"weather_forecast.csv\"\n",
    "forecast_df.to_csv(out_path, index=False)  # index=True 로 하면 인덱스도 같이 저장\n",
    "\n",
    "print(f\"▶ Forecast saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d8770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2hg_time-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
